"""
Custom graders for Spec-Kit evaluation.

These graders check specific quality criteria for specifications and plans
generated by the spec-kit templates.
"""


def check_security_completeness(output: str, context: dict) -> dict:
    """
    Check if security-critical features include proper security requirements.

    Args:
        output: The generated specification or plan text
        context: Additional context (unused but required by PromptFoo)

    Returns:
        dict with 'pass', 'score', and 'reason' keys
    """
    required_security_topics = [
        'authentication',
        'authorization',
        'encryption',
        'session management',
        'data protection',
        'input validation'
    ]

    output_lower = output.lower()
    found_topics = [
        topic for topic in required_security_topics
        if topic in output_lower
    ]

    score = len(found_topics) / len(required_security_topics)

    return {
        'pass': score >= 0.5,  # At least 50% of security topics
        'score': score,
        'reason': f'Found {len(found_topics)}/{len(required_security_topics)} security topics: {", ".join(found_topics) if found_topics else "none"}'
    }


def check_simplicity_gate(output: str, context: dict) -> dict:
    """
    Check if plan follows simplicity gate (Article VII: ≤3 projects).

    Args:
        output: The generated plan text
        context: Additional context (unused but required by PromptFoo)

    Returns:
        dict with 'pass', 'score', and 'reason' keys
    """
    # Count unique projects in the plan
    # Look for patterns like "Project 1:", "## Project", etc.
    import re

    # Extract project numbers to avoid counting duplicates
    # Pattern: "Project" followed by a number
    project_number_pattern = re.compile(r'Project\s+(\d+)', re.IGNORECASE)
    project_numbers = project_number_pattern.findall(output)

    # Get unique project numbers
    unique_projects = set(project_numbers)
    project_count = len(unique_projects)

    # If no numbered projects found, look for "Project Structure" section
    # and try to extract count from table or list
    if project_count == 0:
        # Look for table format: "| Project 1" or "| **Project 1"
        table_project_pattern = re.compile(r'\|\s*\*?\*?Project\s+(\d+)', re.IGNORECASE)
        table_numbers = table_project_pattern.findall(output)
        if table_numbers:
            unique_projects = set(table_numbers)
            project_count = len(unique_projects)

    # If still no projects found, look for explicit project count in text
    if project_count == 0:
        count_pattern = re.compile(r'(\d+)\s+projects?', re.IGNORECASE)
        count_matches = count_pattern.findall(output)
        if count_matches:
            # Take the first explicit count mentioned
            project_count = int(count_matches[0])
        else:
            # Assume single project if nothing found
            project_count = 1

    passed = project_count <= 3
    score = 1.0 if passed else max(0, 1 - (project_count - 3) * 0.2)

    return {
        'pass': passed,
        'score': score,
        'reason': f'Found {project_count} projects (expected ≤3 for simplicity)'
    }


def check_constitution_compliance(output: str, context: dict) -> dict:
    """
    Check if plan violates constitution principles.

    Checks:
    - Article VII: Simplicity (≤3 projects)
    - Article VIII: Anti-Abstraction (no unnecessary wrappers)
    - Over-engineering detection

    Args:
        output: The generated plan text
        context: Additional context (unused but required by PromptFoo)

    Returns:
        dict with 'pass', 'score', and 'reason' keys
    """
    violations = []
    output_lower = output.lower()

    # Check simplicity gate
    simplicity_result = check_simplicity_gate(output, context)
    if not simplicity_result['pass']:
        violations.append(simplicity_result['reason'])

    # Check for over-engineering patterns (context-aware)
    # Only flag if NOT in a negative context (e.g., "no microservices", "avoiding kubernetes")
    import re

    over_engineering_terms = [
        'microservices',
        'kubernetes',
        'k8s',
        'service mesh',
        'event sourcing',
        'cqrs',
        'saga pattern',
        'message queue' if 'simple' in context.get('vars', {}).get('user_input', '').lower() else None
    ]
    over_engineering_terms = [t for t in over_engineering_terms if t]  # Remove None

    found_overengineering = []
    for term in over_engineering_terms:
        if term not in output_lower:
            continue

        # Check if term is in a negative context
        # Look for patterns like "no X", "avoid X", "not X", "without X"
        negative_patterns = [
            rf'\b(no|avoid|avoiding|not|without)\s+\w*\s*{re.escape(term)}',
            rf'{re.escape(term)}\s*\w*\s*(avoided|rejected|unnecessary)'
        ]

        is_negative = False
        for pattern in negative_patterns:
            if re.search(pattern, output_lower, re.IGNORECASE):
                is_negative = True
                break

        # Only flag if NOT in negative context
        if not is_negative:
            found_overengineering.append(term)

    if found_overengineering:
        violations.append(f"Over-engineering detected: {', '.join(found_overengineering)}")

    # Check for unnecessary abstractions/wrappers
    abstraction_terms = [
        'wrapper',
        'facade',
        'adapter layer',
        'abstraction layer'
    ]

    found_abstractions = [
        term for term in abstraction_terms
        if term in output_lower
    ]

    if found_abstractions:
        violations.append(f"Unnecessary abstractions: {', '.join(found_abstractions)}")

    # Calculate score
    if not violations:
        score = 1.0
    else:
        # Deduct 0.3 per violation, minimum 0
        score = max(0, 1.0 - len(violations) * 0.3)

    return {
        'pass': score >= 0.7,
        'score': score,
        'reason': '; '.join(violations) if violations else 'Constitution compliant'
    }


def check_vague_terms(output: str, context: dict) -> dict:
    """
    Check for vague, unmeasurable terms that need clarification.

    Args:
        output: The generated specification text
        context: Additional context (unused but required by PromptFoo)

    Returns:
        dict with 'pass', 'score', and 'reason' keys
    """
    vague_terms = [
        'fast',
        'quick',
        'scalable',
        'secure',
        'intuitive',
        'robust',
        'performant',
        'user-friendly',
        'easy',
        'simple',
        'good performance',
        'high availability'
    ]

    output_lower = output.lower()
    vague_found = [term for term in vague_terms if term in output_lower]

    if not vague_found:
        return {
            'pass': True,
            'score': 1.0,
            'reason': 'No vague terms found'
        }

    # Check if vague terms are quantified or flagged
    quantified_count = 0
    for term in vague_found:
        # Look for the term followed by quantification or clarification markers
        term_index = output_lower.find(term)
        if term_index == -1:
            continue

        # Check 200 chars after the term
        context_window = output_lower[term_index:term_index + 200]

        # Check for quantification patterns
        quantification_patterns = [
            r'\d+\s*(ms|milliseconds|seconds|minutes)',  # time
            r'\d+\s*(mb|gb|requests|users)',  # size/count
            r'<\s*\d+',  # less than X
            r'>\s*\d+',  # greater than X
            r'\[needs clarification\]',
            r'\[tbd\]',
            r'\[todo\]'
        ]

        import re
        if any(re.search(pattern, context_window) for pattern in quantification_patterns):
            quantified_count += 1

    quantified_ratio = quantified_count / len(vague_found) if vague_found else 1.0

    return {
        'pass': quantified_ratio >= 0.7,
        'score': quantified_ratio,
        'reason': f'Found {len(vague_found)} vague terms, {quantified_count} properly quantified/flagged'
    }


def check_edge_cases_coverage(output: str, context: dict) -> dict:
    """
    Check if edge cases section has comprehensive coverage.

    Args:
        output: The generated specification text
        context: Additional context with vars (user_input)

    Returns:
        dict with 'pass', 'score', and 'reason' keys
    """
    import re

    output_lower = output.lower()

    # Define categories of edge cases to check
    edge_case_categories = {
        'boundary_values': [
            'empty', 'min', 'max', 'limit', 'boundary', 'zero', 'negative',
            'very large', 'exceed'
        ],
        'invalid_inputs': [
            'invalid', 'malformed', 'incorrect', 'wrong', 'unsupported',
            'malicious', 'corrupt'
        ],
        'network_failures': [
            'network', 'timeout', 'connection', 'disconnect', 'offline',
            'latency', 'fail'
        ],
        'concurrent_actions': [
            'concurrent', 'simultaneous', 'parallel', 'race condition',
            'multiple users'
        ],
        'state_issues': [
            'session', 'expire', 'recovery', 'rollback', 'partial',
            'inconsistent', 'state'
        ]
    }

    # Count how many categories are covered
    covered_categories = 0
    found_terms = []

    for category, terms in edge_case_categories.items():
        for term in terms:
            if term in output_lower:
                covered_categories += 1
                found_terms.append(f"{category}: {term}")
                break  # Count each category only once

    # Calculate score based on coverage
    total_categories = len(edge_case_categories)
    score = covered_categories / total_categories

    # Pass if at least 3 out of 5 categories covered
    passed = covered_categories >= 3

    return {
        'pass': passed,
        'score': score,
        'reason': f'Covered {covered_categories}/{total_categories} edge case categories ({", ".join(found_terms[:3])}{"..." if len(found_terms) > 3 else ""})'
    }


def check_testability(output: str, context: dict) -> dict:
    """
    Check if requirements are testable with clear acceptance criteria.

    Args:
        output: The generated specification text
        context: Additional context (unused but required by PromptFoo)

    Returns:
        dict with 'pass', 'score', and 'reason' keys
    """
    # Look for acceptance criteria patterns
    import re

    # Find user stories (should have acceptance criteria)
    user_story_pattern = re.compile(r'\*\*As a .+?\*\*', re.IGNORECASE)
    user_stories = user_story_pattern.findall(output)

    if not user_stories:
        return {
            'pass': False,
            'score': 0.0,
            'reason': 'No user stories found'
        }

    # Look for acceptance criteria after each user story
    acceptance_patterns = [
        r'acceptance criteria',
        r'given .+? when .+? then',  # BDD format
        r'should .+?',
        r'must .+?',
        r'verify that',
        r'confirm that'
    ]

    stories_with_criteria = 0
    for story in user_stories:
        # Find the story position
        story_index = output.lower().find(story.lower())
        if story_index == -1:
            continue

        # Check next 500 chars for acceptance criteria
        context_window = output.lower()[story_index:story_index + 500]

        if any(re.search(pattern, context_window, re.IGNORECASE) for pattern in acceptance_patterns):
            stories_with_criteria += 1

    testability_ratio = stories_with_criteria / len(user_stories)

    return {
        'pass': testability_ratio >= 0.7,
        'score': testability_ratio,
        'reason': f'{stories_with_criteria}/{len(user_stories)} user stories have testable acceptance criteria'
    }

def check_clarification_quality(output: str, context: dict) -> dict:
    """
    Check if the clarification report has the required sections and content.
    """
    output_lower = output.lower()
    score = 0
    reasons = []

    if 'ambiguity analysis' in output_lower:
        score += 0.4
        reasons.append("Found 'Ambiguity Analysis' section.")
    else:
        reasons.append("Missing 'Ambiguity Analysis' section.")

    if 'clarification questions' in output_lower:
        score += 0.4
        reasons.append("Found 'Clarification Questions' section.")
    else:
        reasons.append("Missing 'Clarification Questions' section.")

    if 'suggested defaults' in output_lower:
        score += 0.2
        reasons.append("Found 'Suggested Defaults' section.")
    else:
        reasons.append("Missing 'Suggested Defaults' section.")

    return {
        'pass': score > 0.7,
        'score': score,
        'reason': ' '.join(reasons)
    }

def check_architectural_focus(output: str, context: dict) -> dict:
    """
    Check if the clarification questions have an architectural focus.
    """
    output_lower = output.lower()
    score = 0
    reasons = []

    architectural_keywords = ['real-time', 'concurrent users', 'scaling', 'monolith', 'django', 'postgresql']
    found_keywords = [kw for kw in architectural_keywords if kw in output_lower]

    if len(found_keywords) >= 3:
        score = 1.0
        reasons.append(f"Found several architectural keywords: {', '.join(found_keywords)}.")
    elif len(found_keywords) > 0:
        score = 0.5
        reasons.append(f"Found some architectural keywords: {', '.join(found_keywords)}.")
    else:
        score = 0.0
        reasons.append("No architectural keywords found.")

    return {
        'pass': score > 0.7,
        'score': score,
        'reason': ' '.join(reasons)
    }


def check_completeness(output: str, context: dict) -> dict:
    """
    Check if specification has comprehensive coverage of requirements.
    Used for complex features like e-commerce checkout.

    Args:
        output: The generated specification text
        context: Additional context with vars (user_input)

    Returns:
        dict with 'pass', 'score', and 'reason' keys
    """
    import re

    output_lower = output.lower()
    scores = []
    details = []

    # 1. Check for functional requirements section with numbered items
    fr_pattern = re.compile(r'fr-\d+|functional requirement', re.IGNORECASE)
    has_functional_reqs = bool(fr_pattern.search(output))
    if has_functional_reqs:
        scores.append(1.0)
        details.append('functional requirements present')
    else:
        scores.append(0.0)
        details.append('missing functional requirements')

    # 2. Check for user stories (at least 3)
    user_story_pattern = re.compile(r'as a .+?, i want', re.IGNORECASE)
    user_stories = user_story_pattern.findall(output)
    story_score = min(1.0, len(user_stories) / 3)  # Full score at 3+ stories
    scores.append(story_score)
    details.append(f'{len(user_stories)} user stories')

    # 3. Check for non-functional requirements
    nfr_terms = ['performance', 'security', 'scalability', 'availability', 'nfr-']
    nfr_found = sum(1 for term in nfr_terms if term in output_lower)
    nfr_score = min(1.0, nfr_found / 2)  # Full score at 2+ NFR topics
    scores.append(nfr_score)
    details.append(f'{nfr_found} NFR topics')

    # 4. Check for edge cases section
    edge_case_terms = ['edge case', 'error', 'failure', 'timeout', 'invalid', 'exception']
    edge_found = sum(1 for term in edge_case_terms if term in output_lower)
    edge_score = min(1.0, edge_found / 2)  # Full score at 2+ edge case terms
    scores.append(edge_score)
    details.append(f'{edge_found} edge case terms')

    # 5. Check for specific domain terms based on user input
    user_input = context.get('vars', {}).get('user_input', '').lower()

    # For e-commerce checkout, check specific terms
    if 'checkout' in user_input or 'cart' in user_input or 'payment' in user_input:
        ecommerce_terms = ['cart', 'payment', 'order', 'checkout', 'confirmation', 'inventory']
        ecommerce_found = sum(1 for term in ecommerce_terms if term in output_lower)
        domain_score = min(1.0, ecommerce_found / 3)  # Full score at 3+ domain terms
        scores.append(domain_score)
        details.append(f'{ecommerce_found}/6 e-commerce terms')

    # Calculate average score
    avg_score = sum(scores) / len(scores) if scores else 0.0

    return {
        'pass': avg_score >= 0.6,
        'score': avg_score,
        'reason': f'Completeness: {avg_score:.0%} ({", ".join(details)})'
    }
