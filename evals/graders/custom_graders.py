"""
Custom graders for Spec-Kit evaluation.

These graders check specific quality criteria for specifications and plans
generated by the spec-kit templates.
"""


def check_security_completeness(output: str, context: dict) -> dict:
    """
    Check if security-critical features include proper security requirements.

    Args:
        output: The generated specification or plan text
        context: Additional context (unused but required by PromptFoo)

    Returns:
        dict with 'pass', 'score', and 'reason' keys
    """
    required_security_topics = [
        'authentication',
        'authorization',
        'encryption',
        'session management',
        'data protection',
        'input validation'
    ]

    output_lower = output.lower()
    found_topics = [
        topic for topic in required_security_topics
        if topic in output_lower
    ]

    score = len(found_topics) / len(required_security_topics)

    return {
        'pass': score >= 0.5,  # At least 50% of security topics
        'score': score,
        'reason': f'Found {len(found_topics)}/{len(required_security_topics)} security topics: {", ".join(found_topics) if found_topics else "none"}'
    }


def check_simplicity_gate(output: str, context: dict) -> dict:
    """
    Check if plan follows simplicity gate (Article VII: ≤3 projects).

    Args:
        output: The generated plan text
        context: Additional context (unused but required by PromptFoo)

    Returns:
        dict with 'pass', 'score', and 'reason' keys
    """
    # Count projects in the plan
    # Look for patterns like "Project 1:", "## Project", etc.
    import re

    # Pattern 1: "Project N:" or "Project: Name"
    project_pattern1 = re.compile(r'(?:^|\n)(?:#+\s*)?Project\s*(?:\d+|:)', re.IGNORECASE)
    # Pattern 2: "## ProjectName" or "# ProjectName"
    project_pattern2 = re.compile(r'(?:^|\n)#+\s*\w+Project\w*', re.IGNORECASE)

    matches1 = project_pattern1.findall(output)
    matches2 = project_pattern2.findall(output)

    # Take max to avoid undercounting
    project_count = max(len(matches1), len(matches2))

    # If no clear project markers, assume single project
    if project_count == 0:
        project_count = 1

    passed = project_count <= 3
    score = 1.0 if passed else max(0, 1 - (project_count - 3) * 0.2)

    return {
        'pass': passed,
        'score': score,
        'reason': f'Found {project_count} projects (expected ≤3 for simplicity)'
    }


def check_constitution_compliance(output: str, context: dict) -> dict:
    """
    Check if plan violates constitution principles.

    Checks:
    - Article VII: Simplicity (≤3 projects)
    - Article VIII: Anti-Abstraction (no unnecessary wrappers)
    - Over-engineering detection

    Args:
        output: The generated plan text
        context: Additional context (unused but required by PromptFoo)

    Returns:
        dict with 'pass', 'score', and 'reason' keys
    """
    violations = []
    output_lower = output.lower()

    # Check simplicity gate
    simplicity_result = check_simplicity_gate(output, context)
    if not simplicity_result['pass']:
        violations.append(simplicity_result['reason'])

    # Check for over-engineering patterns
    over_engineering_terms = [
        'microservices',
        'kubernetes',
        'k8s',
        'service mesh',
        'event sourcing',
        'cqrs',
        'saga pattern',
        'message queue' if 'simple' in context.get('vars', {}).get('user_input', '').lower() else None
    ]
    over_engineering_terms = [t for t in over_engineering_terms if t]  # Remove None

    found_overengineering = [
        term for term in over_engineering_terms
        if term in output_lower
    ]

    if found_overengineering:
        violations.append(f"Over-engineering detected: {', '.join(found_overengineering)}")

    # Check for unnecessary abstractions/wrappers
    abstraction_terms = [
        'wrapper',
        'facade',
        'adapter layer',
        'abstraction layer'
    ]

    found_abstractions = [
        term for term in abstraction_terms
        if term in output_lower
    ]

    if found_abstractions:
        violations.append(f"Unnecessary abstractions: {', '.join(found_abstractions)}")

    # Calculate score
    if not violations:
        score = 1.0
    else:
        # Deduct 0.3 per violation, minimum 0
        score = max(0, 1.0 - len(violations) * 0.3)

    return {
        'pass': score >= 0.7,
        'score': score,
        'reason': '; '.join(violations) if violations else 'Constitution compliant'
    }


def check_vague_terms(output: str, context: dict) -> dict:
    """
    Check for vague, unmeasurable terms that need clarification.

    Args:
        output: The generated specification text
        context: Additional context (unused but required by PromptFoo)

    Returns:
        dict with 'pass', 'score', and 'reason' keys
    """
    vague_terms = [
        'fast',
        'quick',
        'scalable',
        'secure',
        'intuitive',
        'robust',
        'performant',
        'user-friendly',
        'easy',
        'simple',
        'good performance',
        'high availability'
    ]

    output_lower = output.lower()
    vague_found = [term for term in vague_terms if term in output_lower]

    if not vague_found:
        return {
            'pass': True,
            'score': 1.0,
            'reason': 'No vague terms found'
        }

    # Check if vague terms are quantified or flagged
    quantified_count = 0
    for term in vague_found:
        # Look for the term followed by quantification or clarification markers
        term_index = output_lower.find(term)
        if term_index == -1:
            continue

        # Check 200 chars after the term
        context_window = output_lower[term_index:term_index + 200]

        # Check for quantification patterns
        quantification_patterns = [
            r'\d+\s*(ms|milliseconds|seconds|minutes)',  # time
            r'\d+\s*(mb|gb|requests|users)',  # size/count
            r'<\s*\d+',  # less than X
            r'>\s*\d+',  # greater than X
            r'\[needs clarification\]',
            r'\[tbd\]',
            r'\[todo\]'
        ]

        import re
        if any(re.search(pattern, context_window) for pattern in quantification_patterns):
            quantified_count += 1

    quantified_ratio = quantified_count / len(vague_found) if vague_found else 1.0

    return {
        'pass': quantified_ratio >= 0.7,
        'score': quantified_ratio,
        'reason': f'Found {len(vague_found)} vague terms, {quantified_count} properly quantified/flagged'
    }


def check_testability(output: str, context: dict) -> dict:
    """
    Check if requirements are testable with clear acceptance criteria.

    Args:
        output: The generated specification text
        context: Additional context (unused but required by PromptFoo)

    Returns:
        dict with 'pass', 'score', and 'reason' keys
    """
    # Look for acceptance criteria patterns
    import re

    # Find user stories (should have acceptance criteria)
    user_story_pattern = re.compile(r'\*\*As a .+?\*\*', re.IGNORECASE)
    user_stories = user_story_pattern.findall(output)

    if not user_stories:
        return {
            'pass': False,
            'score': 0.0,
            'reason': 'No user stories found'
        }

    # Look for acceptance criteria after each user story
    acceptance_patterns = [
        r'acceptance criteria',
        r'given .+? when .+? then',  # BDD format
        r'should .+?',
        r'must .+?',
        r'verify that',
        r'confirm that'
    ]

    stories_with_criteria = 0
    for story in user_stories:
        # Find the story position
        story_index = output.lower().find(story.lower())
        if story_index == -1:
            continue

        # Check next 500 chars for acceptance criteria
        context_window = output.lower()[story_index:story_index + 500]

        if any(re.search(pattern, context_window, re.IGNORECASE) for pattern in acceptance_patterns):
            stories_with_criteria += 1

    testability_ratio = stories_with_criteria / len(user_stories)

    return {
        'pass': testability_ratio >= 0.7,
        'score': testability_ratio,
        'reason': f'{stories_with_criteria}/{len(user_stories)} user stories have testable acceptance criteria'
    }
