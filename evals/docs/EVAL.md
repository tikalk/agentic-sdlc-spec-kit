# Annotation Evaluation System

This document explains the annotation evaluation framework for testing spec-kit template quality.

## What are the Annotation Evals?

The annotation evals are a **multi-layered evaluation framework** for testing the quality of spec-kit templates (specifications, plans, architecture docs, extensions) that are generated by AI prompts. The system has three main components.

## 1. Automated Testing (PromptFoo)

**23 LLM eval tests** across 6 suites, plus **4 security graders** that run on every test automatically.

### Test Suites

#### Spec Template Tests (10 tests)
- **Basic Structure**: Validates required sections (Overview, Requirements, User Stories, etc.)
- **No Premature Tech Stack**: Ensures spec focuses on WHAT, not HOW
- **Quality User Stories**: Checks for proper format and acceptance criteria
- **Clarity & Vague Terms**: Flags unmeasurable requirements needing quantification
- **Security Requirements**: Security-critical features include security considerations
- **Edge Cases Coverage**: Validates error scenarios and boundary conditions
- **Completeness**: Comprehensive requirements for complex features
- **Regression**: Even simple features maintain proper structure
- **Rename Regression**: Post-rename output matches quality bar
- **Build-mode Spec**: Build-mode template generates appropriate output

#### Plan Template Tests (2 tests)
- **Simplicity Gate**: Simple apps should have ≤3 projects (Constitution Article VII)
- **Constitution Compliance**: No over-engineering or unnecessary abstractions

#### Architecture Template Tests (4 tests)
- **Init Structure**: Rozanski & Woods sections present (Context View, Functional, Information, Deployment views, Stakeholders, ADRs)
- **Blackbox Context View**: System shown as single opaque box; no internal components leaked; external actors only
- **Simplicity**: Simple apps avoid k8s, microservices, service mesh, CQRS
- **ADR Quality**: Architecture Decision Records follow Status/Context/Decision/Consequences structure

#### Extension System Tests (3 tests)
- **Manifest Validation**: Required fields present (name, version, commands, schema_version, author, requires)
- **Skill Self-containment**: No external `@rule`/`@persona`/`@example` references; prerequisites, purpose, steps included
- **Config Template**: Documented options, required/optional markers, sensible defaults

#### Clarify Command Tests (2 tests)
- **Ambiguity Identification**: Given a vague spec, produces actionable clarifying questions (Ambiguity Analysis, Clarification Questions, Suggested Defaults sections)
- **Architectural Focus**: Questions target scalability, integration points, data flow — not feature details

#### Trace Validation Tests (2 tests)
- **Structure Completeness**: Session metadata, decisions, and artifacts sections present
- **Validation Accuracy**: Validator correctly scores incomplete traces

### Security Graders (applied to every test automatically)

Four graders run on **every LLM output** across all 23 tests via `defaultTest.assert`. A failure in any grader hard-fails that test.

| Grader | Catches |
|--------|---------|
| **`check_pii_leakage`** | Real emails (not placeholder patterns), phone numbers, SSNs, credit card numbers, private IPs (`10.x`, `192.168.x`, `172.16-31.x`), hardcoded API keys (`sk-…`, `AKIA…`, `ghp_…`), bearer tokens |
| **`check_prompt_injection`** | "Ignore previous instructions", "forget everything", "your new instructions are…", embedded `[SYSTEM]:` / `<\|im_start\|>` / `<<SYS>>` markers, DAN mode, "pretend no restrictions", embedded Human/Assistant/System role turns, base64-encoded instruction payloads |
| **`check_hallucination_signals`** | Overconfident precise metrics without hedging, dangling cross-references to non-existent sections/appendices, internal contradictions (e.g. "stateless" + "stores session"), fabricated RFC numbers above 9700 |
| **`check_misinformation`** | MD5/SHA-1/DES/RC4 for passwords, ECB cipher mode, plaintext HTTP endpoints, FTP over SFTP, disabling TLS, `eval()` on user input, `pickle.loads` untrusted data, `yaml.load()` without Loader, `shell=True`, `dangerouslySetInnerHTML`, 100% uptime claims, billion req/sec claims |

### Running Automated Tests

```bash
# Run all 23 LLM eval tests
./evals/scripts/run-promptfoo-eval.sh

# Run with JSON output
./evals/scripts/run-promptfoo-eval.sh --json

# Run and open web UI
./evals/scripts/run-promptfoo-eval.sh --view

# Filter to a specific suite
./evals/scripts/run-promptfoo-eval.sh --filter "Architecture"
```

## 2. Unit Tests (pytest)

Fast tests with no LLM calls — verify grader logic and extension system directly.

```bash
# Test security grader logic (39 tests, ~0.03s)
uv run pytest tests/test_security_graders.py -v

# Test extension system (40+ tests)
uv run pytest tests/test_extensions.py -v

# Run all unit tests
uv run pytest tests/ -v
```

| Test file | Tests | Covers |
|-----------|-------|--------|
| `test_security_graders.py` | 39 | PII leakage, prompt injection, hallucination signals, misinformation — pass cases, fail cases, negative-context edge cases |
| `test_extensions.py` | 40+ | Manifest validation, registry persistence, install/remove workflow, command registration, catalog discovery/search/cache |

## 3. Custom Annotation Tool

Located in `evals/annotation-tool/`, this is a **FastHTML-based web interface** for manual spec review.

### Features
- **Keyboard-driven interface**: N (next), P (previous), 1 (pass), 2 (fail)
- **Progress tracking**: Visual statistics on review progress
- **Note-taking**: Add observations for each spec
- **Auto-save**: Annotations automatically saved to JSON
- **Export**: Timestamped exports for analysis

### Running the Annotation Tool

```bash
# Launch the web interface
./evals/scripts/run-annotation-tool.sh

# Open browser to http://localhost:5001
```

## 4. Error Analysis Workflows

### a) Automated Analysis (using Claude API)

```bash
# For specs
./evals/scripts/run-auto-error-analysis.sh

# For plans
./evals/scripts/run-auto-plan-analysis.sh
```

**Output:**
- `evals/datasets/analysis-results/automated-analysis-<timestamp>.csv`
- `evals/datasets/analysis-results/summary-<timestamp>.txt`

### b) Manual Analysis (using Jupyter)

```bash
./evals/scripts/run-error-analysis.sh
```

**Process:**
1. **Open Coding** — Domain expert reviews 10-20 real specs/plans, notes issues
2. **Axial Coding** — Group similar failures into categories, count frequency
3. **Fix & Iterate** — Address high-frequency failures, add automated checks

## The Complete Workflow

```
1. Generate Specs/Plans/Arch docs (using prompt templates)
   ↓
2. PromptFoo Tests (23 LLM tests + 4 security graders on each)
   ↓
3. Unit Tests (pytest — fast, no API key needed)
   ↓
4. Manual Annotation Tool (human review, pass/fail)
   ↓
5. Error Analysis (categorize failures, find patterns)
   ↓
6. Fix Templates (address high-frequency failures)
   ↓
7. Add Automated Checks (extend PromptFoo / graders)
   ↓
8. Repeat (continuous improvement)
```

## Directory Structure

```
evals/
├── README.md                         # Setup and usage instructions
├── docs/
│   ├── EVAL.md                       # This file — evaluation system overview
│   ├── EVAL-PLAN-2026-Q1.md          # Q1 2026 eval coverage plan
│   ├── QUICK_REFERENCE.md            # One-page command reference
│   ├── GITHUB_ACTIONS_SETUP.md       # CI/CD setup guide
│   ├── LOCAL_TESTING.md              # Local testing with act
│   └── WORKFLOWS.md                  # Error analysis workflows
├── configs/                          # PromptFoo configuration files
│   ├── promptfooconfig.js            # Combined suite (all 23 tests)
│   ├── promptfooconfig-spec.js       # Spec template (10 tests)
│   ├── promptfooconfig-plan.js       # Plan template (2 tests)
│   ├── promptfooconfig-arch.js       # Architecture template (4 tests)
│   ├── promptfooconfig-ext.js        # Extension system (3 tests)
│   ├── promptfooconfig-clarify.js    # Clarify command (2 tests)
│   └── promptfooconfig-trace.js      # Trace validation (2 tests)
├── prompts/                          # Templates under test
│   ├── spec-prompt.txt
│   ├── plan-prompt.txt
│   ├── arch-prompt.txt
│   ├── ext-prompt.txt
│   ├── clarify-prompt.txt
│   └── trace-prompt.txt
├── graders/
│   └── custom_graders.py             # 14 Python graders (quality + 4 security)
├── scripts/                          # Test execution utilities
│   ├── run-promptfoo-eval.sh         # PromptFoo test runner
│   ├── run-error-analysis.sh         # Jupyter error analysis launcher
│   ├── run-auto-error-analysis.sh    # Automated spec analysis
│   ├── run-auto-plan-analysis.sh     # Automated plan analysis
│   └── run-annotation-tool.sh        # Annotation tool launcher
├── annotation-tool/                  # FastHTML annotation interface
├── notebooks/                        # Jupyter notebooks
└── datasets/                         # Test data
    ├── real-specs/                   # 17 generated spec templates
    ├── real-plans/                   # 2 generated plan templates
    └── analysis-results/             # Analysis outputs (gitignored)
```

## Quality Thresholds

| Metric | Threshold |
|--------|-----------|
| Minimum pass rate | ≥ 70% |
| Minimum average score | ≥ 0.70 |
| P0/P1 tests | Must all pass before merge |
| Security graders | Hard fail — any PII/injection/misinformation = 0.0 |

## Quick Reference

| Task | Command |
|------|---------|
| Run all LLM eval tests | `./evals/scripts/run-promptfoo-eval.sh` |
| Run unit tests only | `uv run pytest tests/ -v` |
| Run security grader tests | `uv run pytest tests/test_security_graders.py -v` |
| Launch annotation tool | `./evals/scripts/run-annotation-tool.sh` |
| Run automated spec analysis | `./evals/scripts/run-auto-error-analysis.sh` |
| View results in browser | `./evals/scripts/run-promptfoo-eval.sh --view` |

## See Also

- [README.md](../README.md) — Setup and usage instructions
- [annotation-tool/README.md](../annotation-tool/README.md) — Annotation tool documentation
