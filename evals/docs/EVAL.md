# Annotation Evaluation System

This document explains the annotation evaluation framework for testing spec-kit template quality.

## What are the Annotation Evals?

The annotation evals are a **multi-layered evaluation framework** for testing the quality of spec-kit templates (specifications and implementation plans) that are generated by AI prompts. The system has three main components:

## 1. Automated Testing (PromptFoo)

**10 automated regression tests** that run against the spec and plan generation templates.

**Current Status:** 90% pass rate (9/10 tests passing)

### Test Categories

#### Spec Template Tests (8 tests)
- **Basic Structure**: Validates required sections (Overview, Requirements, User Stories, etc.)
- **No Premature Tech Stack**: Ensures spec focuses on WHAT, not HOW
- **Quality User Stories**: Checks for proper format and acceptance criteria
- **Clarity & Vague Terms**: Flags unmeasurable requirements needing quantification
- **Security Requirements**: Security-critical features include security considerations
- **Edge Cases Coverage**: Validates error scenarios and boundary conditions
- **Completeness**: Comprehensive requirements for complex features
- **Regression**: Even simple features maintain proper structure

#### Plan Template Tests (2 tests)
- **Simplicity Gate**: Simple apps should have ≤3 projects (Constitution Article VII)
- **Constitution Compliance**: No over-engineering or unnecessary abstractions

### Running Automated Tests

```bash
# Run all tests
./evals/scripts/run-promptfoo-eval.sh

# Run with JSON output
./evals/scripts/run-promptfoo-eval.sh --json

# Run and open web UI
./evals/scripts/run-promptfoo-eval.sh --view

# Filter specific tests
./evals/scripts/run-promptfoo-eval.sh --filter "Spec Template"
```

## 2. Custom Annotation Tool

Located in `evals/annotation-tool/`, this is a **FastHTML-based web interface** for manual spec review that's "10x faster than manual review".

### Features
- **Keyboard-driven interface**: N (next), P (previous), 1 (pass), 2 (fail)
- **Progress tracking**: Visual statistics on review progress
- **Note-taking**: Add observations for each spec
- **Auto-save**: Annotations automatically saved to JSON
- **Export**: Timestamped exports for analysis

### Purpose
- Human reviewers manually evaluate generated specs for quality
- Binary pass/fail assessment (no Likert scales)
- Identify failure patterns that automated tests miss
- Build understanding of quality dimensions

### Running the Annotation Tool

```bash
# Launch the web interface
./evals/scripts/run-annotation-tool.sh

# Open browser to http://localhost:5001
```

### Output
Annotations are saved to:
- `evals/annotation-tool/annotations.json` - Current state (auto-saved)
- `evals/annotation-tool/annotations_export_YYYYMMDD_HHMMSS.json` - Timestamped exports

## 3. Error Analysis Workflows

Two approaches for deeper investigation of failures:

### a) Automated Analysis (using Claude API)

Uses Claude API to automatically evaluate specs/plans and categorize failures.

```bash
# For specs
./evals/scripts/run-auto-error-analysis.sh

# For plans
./evals/scripts/run-auto-plan-analysis.sh
```

**Features:**
- Evaluates all specs automatically using Claude
- Binary pass/fail with reasoning
- Categorizes failures automatically
- Generates comprehensive CSV reports and summaries
- Saves time on initial review

**Output:**
- `evals/datasets/analysis-results/automated-analysis-<timestamp>.csv`
- `evals/datasets/analysis-results/summary-<timestamp>.txt`
- `evals/datasets/analysis-results/plan-analysis-<timestamp>.csv`
- `evals/datasets/analysis-results/plan-summary-<timestamp>.txt`

### b) Manual Analysis (using Jupyter)

Deep investigation following qualitative coding methodology.

```bash
# Launch Jupyter Lab
./evals/scripts/run-error-analysis.sh
```

**Process:**
1. **Open Coding** - Domain expert reviews 10-20 real specs/plans, notes issues without categorization
2. **Axial Coding** - Group similar failures into categories, count frequency
3. **Fix & Iterate** - Address high-frequency failures, add automated checks

**Location:** `evals/notebooks/error-analysis.ipynb`

## What Do They Test?

The evaluations test the quality of **AI-generated specifications and implementation plans**.

### Test Data

- **17 diverse spec templates**: `evals/datasets/real-specs/spec-001.md` through `spec-017.md`
- **2 plan templates**: `evals/datasets/real-plans/plan-001.md` and `plan-002.md` (expandable)

### Quality Criteria

#### For Specifications
- Required sections present (Overview, Requirements, User Stories, etc.)
- No premature technical decisions (focuses on WHAT, not HOW)
- Proper user story format with acceptance criteria
- Measurable requirements (flags vague terms)
- Security considerations for security-critical features
- Edge cases and error scenarios
- Completeness for complex features
- Proper structure even for simple features

#### For Implementation Plans
- **Simplicity gate**: ≤3 projects for simple apps (CRITICAL - Constitution Article VII)
- No over-engineering or microservices for simple apps
- Clear project boundaries and tasks
- Testing strategy included
- Appropriate architecture
- No premature optimization
- Testability and verification steps

### Common Failure Categories

**For Specs:**
- Incomplete requirements
- Ambiguous specifications
- Missing acceptance criteria
- Security considerations missing
- Over-engineering indicators
- Vague or unmeasurable terms
- Missing edge cases

**For Plans:**
- Too many projects (>3)
- Over-engineering
- Unclear project boundaries
- Missing verification steps
- Microservices for simple app
- Premature optimization
- Missing testing strategy
- Tech stack mismatch

## The Complete Workflow

```
1. Generate Specs/Plans (using prompt templates)
   ↓
2. PromptFoo Tests (automated regression checks)
   ↓
3. Manual Annotation Tool (human review, pass/fail)
   ↓
4. Error Analysis (categorize failures, find patterns)
   ↓
5. Fix Templates (address high-frequency failures)
   ↓
6. Add Automated Checks (extend PromptFoo)
   ↓
7. Repeat (continuous improvement)
```

## The 80/20 Rule

According to AI evaluation best practices:

> "We spent 60-80% of our development time on error analysis and evaluation. Expect most of your effort to go toward understanding failures (i.e. looking at data) rather than building automated checks."

**80% of value comes from:**
1. Manual annotation tool for human review
2. Error analysis (Jupyter notebooks + automated analysis)
3. PromptFoo for CI/CD regression testing

**20% of value comes from:**
- Production monitoring (planned)
- Advanced features (clustering, AI assistance)

## Key Insight

**Manual review via the annotation tool is the most valuable activity**, with automated tests serving as regression checks to prevent known failures from recurring. The workflow emphasizes:

1. Human judgment first (annotation tool)
2. Pattern discovery (error analysis)
3. Automation second (PromptFoo tests)

This mirrors industry best practices where teams spend 60-80% of evaluation time on manual review and understanding failures rather than building automated checks.

## Directory Structure

```
evals/
├── EVAL.md                    # This file - evaluation system overview
├── README.md                  # Setup and usage instructions
├── configs/                   # PromptFoo configuration files
│   ├── promptfooconfig.js            # Main config (all 10 tests)
│   ├── promptfooconfig-spec.js       # Spec template tests only
│   └── promptfooconfig-plan.js       # Plan template tests only
├── prompts/                   # Templates under test
│   ├── spec-prompt.txt               # Specification generation template
│   └── plan-prompt.txt               # Implementation plan template
├── graders/                   # Custom evaluation logic
│   └── custom_graders.py             # Python-based quality checks
├── scripts/                   # Test execution utilities
│   ├── run-promptfoo-eval.sh         # PromptFoo test runner
│   ├── run-error-analysis.sh         # Jupyter error analysis launcher
│   ├── run-auto-error-analysis.sh    # Automated spec analysis
│   ├── run-auto-plan-analysis.sh     # Automated plan analysis
│   └── run-annotation-tool.sh        # Annotation tool launcher
├── annotation-tool/           # FastHTML annotation interface
│   ├── app.py                        # Web application
│   ├── README.md                     # Tool documentation
│   └── annotations.json              # Saved annotations
├── notebooks/                 # Jupyter notebooks
│   └── error-analysis.ipynb          # Manual review workflow
└── datasets/                  # Test data and results
    ├── real-specs/                   # Generated specs (17 templates)
    ├── real-plans/                   # Generated plans (2 templates)
    └── analysis-results/             # Analysis outputs (CSV, summaries)
```

## Quick Reference

| Task | Command |
|------|---------|
| Run all automated tests | `./evals/scripts/run-promptfoo-eval.sh` |
| Launch annotation tool | `./evals/scripts/run-annotation-tool.sh` |
| Run automated spec analysis | `./evals/scripts/run-auto-error-analysis.sh` |
| Run automated plan analysis | `./evals/scripts/run-auto-plan-analysis.sh` |
| Manual error analysis | `./evals/scripts/run-error-analysis.sh` |
| View test results in browser | `./evals/scripts/run-promptfoo-eval.sh --view` |

## See Also

- [README.md](../README.md) - Complete setup and configuration guide
- [annotation-tool/README.md](../annotation-tool/README.md) - Annotation tool detailed documentation